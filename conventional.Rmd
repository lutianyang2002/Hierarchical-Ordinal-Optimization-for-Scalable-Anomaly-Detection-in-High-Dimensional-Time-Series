---
title: "conventional"
output: html_document
---

Randomized RNN:
```{r, echo=TRUE, warning=FALSE, message=FALSE}
library(torch)
library(dplyr)

data1 <- read.csv("Synthetic_High-Dimensional_Time_Series_Data.csv")

# Preprocessing
features <- as.matrix(data1[, 1:(ncol(data1) - 1)])
result <- as.numeric(data1$Result)
features <- scale(features)
features_tensor <- torch_tensor(features, dtype = torch_float())
result_tensor <- torch_tensor(result, dtype = torch_float())

# Define main function (added seq_len)
rnn_conventional <- function(hidden_size, lr, dropout_rate, epochs, batch_size,
                             vote_rounds = 100, seq_len = 1) {
  vote_count <- rep(0, nrow(features)) 

  # Define RNN model
  RNN_Model <- nn_module(
    initialize = function(input_size, hidden_size, output_size) {
      self$rnn <- nn_rnn(input_size = input_size, hidden_size = hidden_size, batch_first = TRUE)
      self$dropout <- nn_dropout(p = dropout_rate)
      self$fc <- nn_linear(hidden_size, output_size)
    },
    forward = function(x) {
      out <- self$rnn(x)
      out <- self$dropout(out[[1]][, -1, ])  # last hidden output
      out <- self$fc(out)
      out
    }
  )

  # ==== Handle sequence length ====
  if (seq_len > 1) {
    # Sliding window segmentation
    num_segments <- nrow(features) - seq_len + 1
    X_seq <- array(0, dim = c(num_segments, seq_len, ncol(features)))
    y_seq <- numeric(num_segments)
    for (i in 1:num_segments) {
      X_seq[i, , ] <- features[i:(i + seq_len - 1), ]
      y_seq[i] <- result[i + seq_len - 1]
    }
    X_tensor <- torch_tensor(X_seq, dtype = torch_float())
    y_tensor <- torch_tensor(y_seq, dtype = torch_float())
  } else {
    # Original single-point case (same as before)
    X_tensor <- features_tensor$view(c(nrow(features), 1, ncol(features)))
    y_tensor <- result_tensor
  }

  # ==== Voting rounds ====
  for (r in 1:vote_rounds) {
    set.seed(r)
    torch_manual_seed(r)
    model <- RNN_Model(ncol(features), hidden_size, 1)
    opt <- optim_adam(model$parameters, lr = lr)
    loss_fn <- nn_mse_loss()

    for (epoch in 1:epochs) {
      model$train()
      opt$zero_grad()
      pred <- model(X_tensor)
      loss <- loss_fn(pred$squeeze(), y_tensor)
      loss$backward()
      opt$step()
    }

    model$eval()
    pred <- model(X_tensor)
    pred_values <- as.numeric(pred$squeeze())
    errors <- abs(as.numeric(y_tensor) - pred_values)

    # Select top 5% (10 points) with the largest errors in this round
    top10 <- order(errors, decreasing = TRUE)[1:10]
    vote_count[top10] <- vote_count[top10] + 1
  }

  # Final anomalies: 10 points with the highest total votes
  final_anomalies <- order(vote_count, decreasing = TRUE)[1:10]
  return(sort(final_anomalies))
}

# Ground-truth anomalies
true_anomalies <- c(8, 92, 122, 127, 130, 149, 150, 159, 164, 196)

# Compute precision
compute_precision <- function(detected, truth) {
  tp <- sum(detected %in% truth)
  fp <- length(detected) - tp
  precision <- tp / (tp + fp)
  return(precision)
}

# Conventional Model 1
result1_rnn <- rnn_conventional(
  hidden_size = 75,
  lr = 0.006,
  dropout_rate = 0.15,
  epochs = 150,
  batch_size = 80,
  vote_rounds = 100,
  seq_len = 30
)
prec1_rnn <- compute_precision(result1_rnn, true_anomalies)
cat("Precision =", round(prec1_rnn, 4), "\n")

# Conventional Model 2
result2_rnn <- rnn_conventional(
  hidden_size = 85,
  lr = 0.0045,
  dropout_rate = 0.12,
  epochs = 200,
  batch_size = 96,
  vote_rounds = 100,
  seq_len = 25
)
prec2_rnn <- compute_precision(result2_rnn, true_anomalies)
cat("Precision =", round(prec2_rnn, 4), "\n")

# Conventional Model 3
result3_rnn <- rnn_conventional(
  hidden_size = 90,
  lr = 0.004,
  dropout_rate = 0.08,
  epochs = 220,
  batch_size = 112,
  vote_rounds = 100,
  seq_len = 15
)
prec3_rnn <- compute_precision(result3_rnn, true_anomalies)
cat("Precision =", round(prec3_rnn, 4), "\n")
```


Particle Filter:
```{r, echo=TRUE, warning=FALSE, message=FALSE}
library(torch)
library(dplyr)

data1 <- read.csv("Synthetic_High-Dimensional_Time_Series_Data.csv")

# Preprocessing
features <- as.matrix(data1[, 1:(ncol(data1) - 1)])
result <- as.numeric(data1$Result)
features <- scale(features)
features_tensor <- torch_tensor(features, dtype = torch_float())
result_tensor <- torch_tensor(result, dtype = torch_float())

# Define main function
pf_conventional <- function(num_particles, sigma_process, sigma_measure) {
  n <- length(result)
  particles <- rnorm(num_particles, mean = result[1], sd = sigma_process)
  weights <- rep(1 / num_particles, num_particles)
  anomaly_scores <- numeric(n)

  for (t in 2:n) {
    particles <- particles + rnorm(num_particles, mean = 0, sd = sigma_process)
    likelihood <- dnorm(result[t], mean = particles, sd = sigma_measure)
    weights <- weights * likelihood
    weights <- weights / sum(weights)
    estimate <- sum(particles * weights)
    anomaly_scores[t] <- abs(result[t] - estimate)
    indices <- sample(1:num_particles, num_particles, replace = TRUE, prob = weights)
    particles <- particles[indices]
    weights <- rep(1 / num_particles, num_particles)
  }

  # Top 10 anomalies
  final_anomalies <- order(anomaly_scores, decreasing = TRUE)[1:10]
  return(sort(final_anomalies))
}

# Ground-truth anomalies
true_anomalies <- c(8, 92, 122, 127, 130, 149, 150, 159, 164, 196)

# Compute precision
compute_precision <- function(detected, truth) {
  tp <- sum(detected %in% truth)
  fp <- length(detected) - tp
  precision <- tp / (tp + fp)
  return(precision)
}

# Conventional Model 1
result1_pf <- pf_conventional(
  num_particles = 4000,
  sigma_process = 0.48,
  sigma_measure = 0.67
)
prec1_pf <- compute_precision(result1_pf, true_anomalies)
cat("Precision =", round(prec1_pf, 4), "\n")

# Conventional Model 2
result2_pf <- pf_conventional(
  num_particles = 4500,
  sigma_process = 0.45,
  sigma_measure = 0.75
)
prec2_pf <- compute_precision(result2_pf, true_anomalies)
cat("Precision =", round(prec2_pf, 4), "\n")

# Conventional Model 3
result3_pf <- pf_conventional(
  num_particles = 4400,
  sigma_process = 0.50,
  sigma_measure = 0.69
)
prec3_pf <- compute_precision(result3_pf, true_anomalies)
cat("Precision =", round(prec3_pf, 4), "\n")
```


Isolation Forest:
```{r, echo=TRUE, warning=FALSE, message=FALSE}
library(isotree)
library(dplyr)

data2 <- read.csv("water.csv")

# Define main function
iforest_conventional <- function(data2, num_trees = 3000, sample_size = 224,
                                 ndim = 6, weight_prob = 0.35,
                                 top_k = 1726, vote_rounds = 100) {
  data2.raw <- data2 %>% select(-EVENT)
  n <- nrow(data2)
  vote_matrix <- matrix(0, nrow = n, ncol = vote_rounds)

  for (i in 1:vote_rounds) {
    model <- isolation.forest(
      data2.raw,
      ntrees = num_trees,
      sample_size = sample_size,
      ndim = ndim,
      prob_pick_pooled_gain = weight_prob
    )
    scores <- predict(model, data2.raw, type = "score")
    top_idx <- order(scores, decreasing = TRUE)[1:top_k]
    vote_matrix[top_idx, i] <- 1
  }

  total_votes <- rowSums(vote_matrix)
  final_anomalies <- order(total_votes, decreasing = TRUE)[1:top_k]

  return(sort(final_anomalies))
}

# Compute precision
compute_precision <- function(detected, truth) {
  predicted_labels <- rep(0, length(truth))
  predicted_labels[detected] <- 1
  tp <- sum(predicted_labels == 1 & truth == TRUE)
  fp <- sum(predicted_labels == 1 & truth == FALSE)
  precision <- tp / (tp + fp)
  return(precision)
}

# Ground-truth EVENT labels
true_labels <- data2$EVENT

# Conventional Model 1
result1_iforest <- iforest_conventional(
  data2,
  num_trees = 1000,
  sample_size = 160,
  ndim = 1,
  weight_prob = 0.10,
  top_k = 1726
)
prec1_iforest <- compute_precision(result1_iforest, true_labels)
cat("Precision =", round(prec1_iforest, 4), "\n")

# Conventional Model 2
result2_iforest <- iforest_conventional(
  data2,
  num_trees = 1250,
  sample_size = 192,
  ndim = 2,
  weight_prob = 0.12,
  top_k = 1726
)
prec2_iforest <- compute_precision(result2_iforest, true_labels)
cat("Precision =", round(prec2_iforest, 4), "\n")

# Conventional Model 3
result3_iforest <- iforest_conventional(
  data2,
  num_trees = 800,
  sample_size = 216,
  ndim = 3,
  weight_prob = 0.15,
  top_k = 1726
)
prec3_iforest <- compute_precision(result3_iforest, true_labels)
cat("Precision =", round(prec3_iforest, 4), "\n")
```


Randomized Subspace Clustering: 
```{r, echo=TRUE, warning=FALSE, message=FALSE}
library(dbscan)

data2_raw <- read.csv("water.csv")
data2 <- data2_raw[complete.cases(data2_raw), ]
true_anomalies <- which(data2$EVENT == TRUE)
data2_no_time <- data2[, !(names(data2) %in% c("Time", "EVENT"))]

# Define main function
dbscan_conventional <- function(data2,
                                n_trials = 100,
                                subspace_dim = 6,
                                eps = 0.4,
                                minPts = 6,
                                final_top_k = 1726,
                                trial_top_percent = 0.01,
                                verbose = FALSE) {
  n <- nrow(data2)
  p <- ncol(data2)
  vote_counts <- rep(0, n)
  data2 <- scale(data2)

  for (t in 1:n_trials) {
    selected_features <- sample(1:p, subspace_dim)
    data2_sub <- data2[, selected_features, drop = FALSE]
    set.seed(t)
    dbscan_result <- tryCatch({
      dbscan::dbscan(data2_sub, eps = eps, minPts = minPts)
    }, error = function(e) return(NULL))

    if (is.null(dbscan_result)) next
    cluster_labels <- dbscan_result$cluster
    cluster_sizes <- table(cluster_labels[cluster_labels != 0])
    sorted_clusters <- names(sort(cluster_sizes, decreasing = FALSE))
    cumulative <- 0
    selected_clusters <- c()
    for (cl in sorted_clusters) {
      cumulative <- cumulative + cluster_sizes[cl]
      selected_clusters <- c(selected_clusters, cl)
      if (cumulative >= trial_top_percent * n) break
    }

    trial_anomalies <- which(cluster_labels == 0 | cluster_labels %in% as.integer(selected_clusters))
    vote_counts[trial_anomalies] <- vote_counts[trial_anomalies] + 1

    if (verbose && t %% 10 == 0) {
      cat("Trial", t, "completed\n")
    }
  }

  final_anomalies <- order(vote_counts, decreasing = TRUE)[1:final_top_k]
  return(sort(final_anomalies))
}

# Conventional Model 1
result1_dbscan <- dbscan_conventional(
  data2_no_time,
  n_trials = 100,
  subspace_dim = 3,
  eps = 0.3,
  minPts = 3,
  final_top_k = 1726
)
prec1_dbscan <- length(intersect(result1_dbscan, true_anomalies)) / 1726
cat("Model 1 - Precision:", round(prec1_dbscan, 4), "\n")

# Conventional Model 2
result2_dbscan <- dbscan_conventional(
  data2_no_time,
  n_trials = 100,
  subspace_dim = 4,
  eps = 0.5,
  minPts = 4,
  final_top_k = 1726
)
prec2_dbscan <- length(intersect(result2_dbscan, true_anomalies)) / 1726
cat("Model 2 - Precision:", round(prec2_dbscan, 4), "\n")

# Conventional Model 3
result3_dbscan <- dbscan_conventional(
  data2_no_time,
  n_trials = 100,
  subspace_dim = 5,
  eps = 0.7,
  minPts = 5,
  final_top_k = 1726
)
prec3_dbscan <- length(intersect(result3_dbscan, true_anomalies)) / 1726
cat("Model 3 - Precision:", round(prec3_dbscan, 4), "\n")
```





